{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rex/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Rex Sutton\n",
    "#\n",
    "# Demonstration implementation of Deep Deterministic Policy Gradients, ...\n",
    "#\n",
    "# @article{lillicrap2015continuous,\n",
    "#  title={Continuous control with deep reinforcement learning},\n",
    "#  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},\n",
    "#  journal={arXiv preprint arXiv:1509.02971},\n",
    "#  year={2015}\n",
    "#\n",
    "\n",
    "import collections\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.57319809, -0.81941683,  0.54768334])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# typical starting point for environment\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(1,)\n",
      "[-2.]\n",
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "# action space\n",
    "print(env.action_space)\n",
    "print(env.action_space.low)\n",
    "print(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(3,)\n",
      "[-1. -1. -8.]\n",
      "[1. 1. 8.]\n"
     ]
    }
   ],
   "source": [
    "# control space\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a replay memory\n",
    "replay_memory_size = 10000\n",
    "replay_memory = collections.deque([], maxlen=replay_memory_size)\n",
    "def sample_memories(batch_size):\n",
    "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
    "    cols = [[], [], [], [], []] # state, control, reward, nextstate, continue\n",
    "    for idx in indices:\n",
    "        memory = replay_memory[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actor, given a state returns the controls\n",
    "def make_actor_network(states, num_ctrls, scope_name):\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    kernel_regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        # two layers of relu\n",
    "        hidden_1 = tf.layers.dense(states, units=8,\n",
    "                                   activation=tf.nn.elu,\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   kernel_regularizer=kernel_regularizer)  \n",
    "        hidden_2 = tf.layers.dense(hidden_1, units=8,\n",
    "                                   activation=tf.nn.elu,\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   kernel_regularizer=kernel_regularizer)  \n",
    "        actions = tf.layers.dense(hidden_2, num_ctrls,\n",
    "                                   use_bias=False,\n",
    "                                   activation=tf.nn.tanh,\n",
    "                                   kernel_initializer=initializer)  \n",
    "        # collect the trainable variables\n",
    "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                           scope=scope.name)\n",
    "        trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                                  for var in trainable_vars}\n",
    "    return actions, trainable_vars_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic is estimating the expected rewards of the action in the states\n",
    "def make_critic_network(states, controls, scope_name):\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    kernel_regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "    with tf.variable_scope(scope_name) as scope:\n",
    "        # pass states through relu\n",
    "        hidden_1 = tf.layers.dense(states, units=8,\n",
    "                                   activation=tf.nn.elu,\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   kernel_regularizer=kernel_regularizer)\n",
    "        # another relu layer\n",
    "        hidden_2 = tf.layers.dense(controls, units=8,\n",
    "                                   activation=tf.nn.elu,\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   kernel_regularizer=kernel_regularizer)  \n",
    "        # concatenate the hidden layer with the actions\n",
    "        concat = tf.concat(axis=1, values=[hidden_1, hidden_2])\n",
    "        # another relu layer\n",
    "        hidden_3 = tf.layers.dense(concat, units=16,\n",
    "                                   activation=tf.nn.elu,\n",
    "                                   kernel_initializer=initializer,\n",
    "                                   kernel_regularizer=kernel_regularizer)  \n",
    "        # a linear layer\n",
    "        outputs = tf.layers.dense(hidden_3, 1,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  kernel_regularizer=kernel_regularizer)  \n",
    "    # collect the trainable variables\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                       scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                              for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a soft copy from the online to the target\n",
    "def make_copy_op(target, online):\n",
    "    return target.assign_sub(0.001 * (target - online))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return state\n",
    "tf.set_random_seed(seed=42)\n",
    "np.random.seed(seed=42)\n",
    "replay_memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the graph before defining it\n",
    "tf.reset_default_graph()\n",
    "# problem dimensions\n",
    "state_dim = 3\n",
    "num_ctrls = 1\n",
    "# place holders\n",
    "states = tf.placeholder(tf.float32, (None, state_dim))\n",
    "target_states = tf.placeholder(tf.float32, (None, state_dim))\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "# make online networks\n",
    "actor_outputs, actor_vars = make_actor_network(states, num_ctrls, 'actor')\n",
    "critic_outputs, critic_vars = make_critic_network(states, actor_outputs, 'critic')\n",
    "# make target networks\n",
    "target_actor_outputs, target_actor_vars = make_actor_network(target_states, num_ctrls, 'actor_target')\n",
    "target_critic_outputs, target_critic_vars = make_critic_network(target_states, target_actor_outputs, 'critic_target')\n",
    "# make soft copy operations\n",
    "actor_copy_ops = [make_copy_op(target_actor_vars[var_name], actor_vars[var_name])\n",
    "                    for var_name in target_actor_vars.keys()]\n",
    "critic_copy_ops = [make_copy_op(target_critic_vars[var_name], critic_vars[var_name])\n",
    "                    for var_name in target_critic_vars.keys()]\n",
    "copy_online_to_target = tf.group(*(actor_copy_ops + critic_copy_ops))\n",
    "# train the critic (slight divergence from paper in treat error more carefully)\n",
    "critic_error = tf.abs(y - critic_outputs)\n",
    "clipped_critic_error = tf.clip_by_value(critic_error, 0.0, 1.0)\n",
    "linear_critic_error = 2.0 * (critic_error - clipped_critic_error)\n",
    "critic_loss = tf.reduce_mean(tf.square(clipped_critic_error) + linear_critic_error)\n",
    "critic_optimizer = tf.train.MomentumOptimizer(learning_rate = 0.005, momentum = 0.95, use_nesterov=True)\n",
    "critic_training_op = critic_optimizer.minimize(critic_loss, var_list=list(critic_vars.values()))\n",
    "# train the actor (equivalent to Silver's derivation but easier)\n",
    "neg_mean_q = -1.0 * tf.reduce_mean(critic_outputs)\n",
    "actor_optimizer = tf.train.MomentumOptimizer(learning_rate = 0.01, momentum = 0.95, use_nesterov=True)\n",
    "actor_training_op = actor_optimizer.minimize(neg_mean_q, var_list=list(actor_vars.values()))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "save_path = './pendulumv0.ckpt'\n",
    "max_episodes = 250\n",
    "max_episode_steps = 200\n",
    "max_explore_episodes = 100\n",
    "batch_size = 256\n",
    "min_replays = 3*batch_size\n",
    "noise_theta = 0.15\n",
    "noise_sigma = 0.2\n",
    "discount_rate = 0.99\n",
    "control_scalar = 2.0 # pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0\tStep 200\tTotRwd -1504.175138\tLoss   nan\tAvgQ nan\tMaxRwd -4.719712\n",
      "Ep 1\tStep 200\tTotRwd -1274.403934\tLoss   nan\tAvgQ nan\tMaxRwd -1.553310\n",
      "Ep 2\tStep 200\tTotRwd -1723.612255\tLoss   nan\tAvgQ nan\tMaxRwd -1.553310\n",
      "Ep 3\tStep 200\tTotRwd -1275.167530\tLoss 1.453947\tAvgQ -8.619713\tMaxRwd -0.526186\n",
      "Ep 4\tStep 200\tTotRwd -1649.650098\tLoss 0.177551\tAvgQ -8.015076\tMaxRwd -0.526186\n",
      "Ep 5\tStep 200\tTotRwd -1296.028377\tLoss 0.077982\tAvgQ -8.686527\tMaxRwd -0.526186\n",
      "Ep 6\tStep 200\tTotRwd -994.115728\tLoss 0.107734\tAvgQ -9.061883\tMaxRwd -0.037844\n",
      "Ep 7\tStep 200\tTotRwd -1066.341837\tLoss 0.079162\tAvgQ -10.109612\tMaxRwd -0.037844\n",
      "Ep 8\tStep 200\tTotRwd -1505.866620\tLoss 0.038884\tAvgQ -11.098179\tMaxRwd -0.037844\n",
      "Ep 9\tStep 200\tTotRwd -1249.790099\tLoss 0.118232\tAvgQ -12.211815\tMaxRwd -0.037844\n",
      "Ep 10\tStep 200\tTotRwd -1420.985118\tLoss 0.082418\tAvgQ -12.701385\tMaxRwd -0.037844\n",
      "Ep 11\tStep 200\tTotRwd -1213.287248\tLoss 0.092588\tAvgQ -14.150100\tMaxRwd -0.037844\n",
      "Ep 12\tStep 200\tTotRwd -1155.506371\tLoss 0.218009\tAvgQ -14.294854\tMaxRwd -0.037844\n",
      "Ep 13\tStep 200\tTotRwd -1559.240069\tLoss 0.361834\tAvgQ -15.091679\tMaxRwd -0.037844\n",
      "Ep 14\tStep 200\tTotRwd -1043.537399\tLoss 0.370267\tAvgQ -16.635971\tMaxRwd -0.037844\n",
      "Ep 15\tStep 200\tTotRwd -1167.479879\tLoss 0.455642\tAvgQ -16.958118\tMaxRwd -0.037844\n",
      "Ep 16\tStep 200\tTotRwd -1504.563460\tLoss 0.867869\tAvgQ -19.812895\tMaxRwd -0.037844\n",
      "Ep 17\tStep 200\tTotRwd -1697.891369\tLoss 0.957775\tAvgQ -19.938778\tMaxRwd -0.037844\n",
      "Ep 18\tStep 200\tTotRwd -1088.342307\tLoss 0.630264\tAvgQ -22.162880\tMaxRwd -0.037844\n",
      "Ep 19\tStep 200\tTotRwd -863.357793\tLoss 0.714817\tAvgQ -21.725887\tMaxRwd -0.037844\n",
      "Ep 20\tStep 200\tTotRwd -1182.609456\tLoss 0.935101\tAvgQ -25.352369\tMaxRwd -0.037844\n",
      "Ep 21\tStep 200\tTotRwd -1394.655872\tLoss 0.995941\tAvgQ -24.689495\tMaxRwd -0.037844\n",
      "Ep 22\tStep 200\tTotRwd -1609.244156\tLoss 0.584695\tAvgQ -25.277538\tMaxRwd -0.037844\n",
      "Ep 23\tStep 200\tTotRwd -1063.262201\tLoss 0.495221\tAvgQ -26.727089\tMaxRwd -0.018596\n",
      "Ep 24\tStep 200\tTotRwd -1529.230097\tLoss 0.654616\tAvgQ -28.419649\tMaxRwd -0.018596\n",
      "Ep 25\tStep 200\tTotRwd -1253.539966\tLoss 0.626628\tAvgQ -31.158287\tMaxRwd -0.018596\n",
      "Ep 26\tStep 200\tTotRwd -1687.168092\tLoss 1.184304\tAvgQ -29.490761\tMaxRwd -0.018596\n",
      "Ep 27\tStep 200\tTotRwd -1547.813594\tLoss 0.664886\tAvgQ -33.299202\tMaxRwd -0.018596\n",
      "Ep 28\tStep 200\tTotRwd -1199.950482\tLoss 0.446405\tAvgQ -33.994499\tMaxRwd -0.018596\n",
      "Ep 29\tStep 200\tTotRwd -1466.076940\tLoss 1.441107\tAvgQ -34.541164\tMaxRwd -0.018596\n",
      "Ep 30\tStep 200\tTotRwd -1389.213411\tLoss 0.900557\tAvgQ -34.286541\tMaxRwd -0.018596\n",
      "Ep 31\tStep 200\tTotRwd -1507.325956\tLoss 0.369741\tAvgQ -37.090668\tMaxRwd -0.018596\n",
      "Ep 32\tStep 200\tTotRwd -1324.139714\tLoss 1.122887\tAvgQ -37.533661\tMaxRwd -0.018596\n",
      "Ep 33\tStep 200\tTotRwd -1407.444668\tLoss 0.816675\tAvgQ -39.653427\tMaxRwd -0.018596\n",
      "Ep 34\tStep 200\tTotRwd -1447.272526\tLoss 1.255429\tAvgQ -38.875404\tMaxRwd -0.018596\n",
      "Ep 35\tStep 200\tTotRwd -1218.546791\tLoss 1.594990\tAvgQ -42.559200\tMaxRwd -0.018596\n",
      "Ep 36\tStep 200\tTotRwd -1218.976124\tLoss 1.221951\tAvgQ -40.943405\tMaxRwd -0.018596\n",
      "Ep 37\tStep 200\tTotRwd -1223.361732\tLoss 1.161315\tAvgQ -42.003887\tMaxRwd -0.018596\n",
      "Ep 38\tStep 200\tTotRwd -1433.109591\tLoss 0.835470\tAvgQ -43.843533\tMaxRwd -0.018596\n",
      "Ep 39\tStep 200\tTotRwd -1147.800960\tLoss 1.126971\tAvgQ -44.557827\tMaxRwd -0.018596\n",
      "Ep 40\tStep 200\tTotRwd -1117.154690\tLoss 0.284549\tAvgQ -46.028572\tMaxRwd -0.018596\n",
      "Ep 41\tStep 200\tTotRwd -1037.278943\tLoss 1.131813\tAvgQ -47.139114\tMaxRwd -0.018596\n",
      "Ep 42\tStep 200\tTotRwd -1152.403381\tLoss 0.695719\tAvgQ -48.080017\tMaxRwd -0.018596\n",
      "Ep 43\tStep 200\tTotRwd -1295.121037\tLoss 1.561117\tAvgQ -51.394310\tMaxRwd -0.018596\n",
      "Ep 44\tStep 200\tTotRwd -1089.802145\tLoss 1.216507\tAvgQ -48.544701\tMaxRwd -0.018596\n",
      "Ep 45\tStep 200\tTotRwd -1200.544787\tLoss 1.422494\tAvgQ -50.659767\tMaxRwd -0.018596\n",
      "Ep 46\tStep 200\tTotRwd -1257.757179\tLoss 1.966415\tAvgQ -52.857250\tMaxRwd -0.018596\n",
      "Ep 47\tStep 200\tTotRwd -977.353472\tLoss 1.252697\tAvgQ -51.899464\tMaxRwd -0.010830\n",
      "Ep 48\tStep 200\tTotRwd -1225.981401\tLoss 1.169532\tAvgQ -54.577293\tMaxRwd -0.010830\n",
      "Ep 49\tStep 200\tTotRwd -1193.031857\tLoss 1.364447\tAvgQ -53.401485\tMaxRwd -0.010830\n",
      "Ep 50\tStep 200\tTotRwd -1187.221125\tLoss 0.617582\tAvgQ -56.005398\tMaxRwd -0.010830\n",
      "Ep 51\tStep 200\tTotRwd -619.687183\tLoss 1.891495\tAvgQ -58.887634\tMaxRwd -0.002903\n",
      "Ep 52\tStep 200\tTotRwd -1203.436313\tLoss 1.059983\tAvgQ -56.852409\tMaxRwd -0.002903\n",
      "Ep 53\tStep 200\tTotRwd -1076.655834\tLoss 1.388375\tAvgQ -55.135216\tMaxRwd -0.002903\n",
      "Ep 54\tStep 200\tTotRwd -796.757007\tLoss 2.053468\tAvgQ -54.739391\tMaxRwd -0.002903\n",
      "Ep 55\tStep 200\tTotRwd -1122.382723\tLoss 1.379253\tAvgQ -60.373161\tMaxRwd -0.002903\n",
      "Ep 56\tStep 200\tTotRwd -819.819262\tLoss 1.087373\tAvgQ -58.348083\tMaxRwd -0.002903\n",
      "Ep 57\tStep 200\tTotRwd -381.289376\tLoss 1.992072\tAvgQ -62.816277\tMaxRwd -0.000068\n",
      "Ep 58\tStep 200\tTotRwd -1170.006772\tLoss 1.286450\tAvgQ -61.831116\tMaxRwd -0.000068\n",
      "Ep 59\tStep 200\tTotRwd -506.031981\tLoss 1.664472\tAvgQ -64.333397\tMaxRwd -0.000068\n",
      "Ep 60\tStep 200\tTotRwd -783.790028\tLoss 2.355678\tAvgQ -63.225922\tMaxRwd -0.000068\n",
      "Ep 61\tStep 200\tTotRwd -495.615955\tLoss 2.093558\tAvgQ -59.179871\tMaxRwd -0.000068\n",
      "Ep 62\tStep 200\tTotRwd -748.313272\tLoss 1.655777\tAvgQ -61.678154\tMaxRwd -0.000068\n",
      "Ep 63\tStep 200\tTotRwd -130.461826\tLoss 0.965253\tAvgQ -62.928089\tMaxRwd -0.000068\n",
      "Ep 64\tStep 200\tTotRwd -548.217451\tLoss 2.112637\tAvgQ -59.418674\tMaxRwd -0.000068\n",
      "Ep 65\tStep 200\tTotRwd -842.598203\tLoss 2.083282\tAvgQ -61.174099\tMaxRwd -0.000068\n",
      "Ep 66\tStep 200\tTotRwd -514.703819\tLoss 1.463861\tAvgQ -63.177525\tMaxRwd -0.000068\n",
      "Ep 67\tStep 200\tTotRwd -726.139910\tLoss 2.330609\tAvgQ -59.263977\tMaxRwd -0.000068\n",
      "Ep 68\tStep 200\tTotRwd -254.820231\tLoss 1.912856\tAvgQ -63.996307\tMaxRwd -0.000068\n",
      "Ep 69\tStep 200\tTotRwd -254.791402\tLoss 1.327158\tAvgQ -63.223980\tMaxRwd -0.000068\n",
      "Ep 70\tStep 200\tTotRwd -128.444833\tLoss 2.000164\tAvgQ -64.327492\tMaxRwd -0.000068\n",
      "Ep 71\tStep 200\tTotRwd -374.031832\tLoss 2.161650\tAvgQ -61.985901\tMaxRwd -0.000068\n",
      "Ep 72\tStep 200\tTotRwd -381.874317\tLoss 1.437460\tAvgQ -62.570580\tMaxRwd -0.000068\n",
      "Ep 73\tStep 200\tTotRwd -255.319783\tLoss 2.540662\tAvgQ -61.391098\tMaxRwd -0.000068\n",
      "Ep 74\tStep 200\tTotRwd -128.927400\tLoss 1.642246\tAvgQ -60.746750\tMaxRwd -0.000068\n",
      "Ep 75\tStep 200\tTotRwd -128.684413\tLoss 1.891422\tAvgQ -59.540085\tMaxRwd -0.000039\n",
      "Ep 76\tStep 200\tTotRwd -261.059037\tLoss 1.366967\tAvgQ -59.070328\tMaxRwd -0.000039\n",
      "Ep 77\tStep 200\tTotRwd -237.575429\tLoss 2.620741\tAvgQ -56.589890\tMaxRwd -0.000039\n",
      "Ep 78\tStep 200\tTotRwd -1293.169281\tLoss 1.651620\tAvgQ -57.077141\tMaxRwd -0.000039\n",
      "Ep 79\tStep 200\tTotRwd -1151.950915\tLoss 1.842907\tAvgQ -55.781914\tMaxRwd -0.000039\n",
      "Ep 80\tStep 200\tTotRwd -252.734479\tLoss 1.142776\tAvgQ -56.190643\tMaxRwd -0.000039\n",
      "Ep 81\tStep 200\tTotRwd -378.872016\tLoss 1.608602\tAvgQ -57.629204\tMaxRwd -0.000039\n",
      "Ep 82\tStep 200\tTotRwd -1.762410\tLoss 1.793117\tAvgQ -52.005615\tMaxRwd -0.000039\n",
      "Ep 83\tStep 200\tTotRwd -126.764723\tLoss 1.579404\tAvgQ -53.476585\tMaxRwd -0.000039\n",
      "Ep 84\tStep 200\tTotRwd -241.348641\tLoss 2.342389\tAvgQ -50.501450\tMaxRwd -0.000039\n",
      "Ep 85\tStep 200\tTotRwd -1044.365410\tLoss 1.134306\tAvgQ -51.897167\tMaxRwd -0.000039\n",
      "Ep 86\tStep 200\tTotRwd -130.322792\tLoss 2.874140\tAvgQ -56.503174\tMaxRwd -0.000039\n",
      "Ep 87\tStep 200\tTotRwd -127.614498\tLoss 1.566963\tAvgQ -44.386772\tMaxRwd -0.000039\n",
      "Ep 88\tStep 200\tTotRwd -128.774370\tLoss 1.604857\tAvgQ -50.397831\tMaxRwd -0.000039\n",
      "Ep 89\tStep 200\tTotRwd -2.826417\tLoss 1.689785\tAvgQ -46.859993\tMaxRwd -0.000039\n",
      "Ep 90\tStep 200\tTotRwd -128.141605\tLoss 1.789433\tAvgQ -42.578918\tMaxRwd -0.000039\n",
      "Ep 91\tStep 200\tTotRwd -1503.083733\tLoss 2.147274\tAvgQ -44.724487\tMaxRwd -0.000039\n",
      "Ep 92\tStep 200\tTotRwd -129.795309\tLoss 2.685510\tAvgQ -44.065800\tMaxRwd -0.000039\n",
      "Ep 93\tStep 200\tTotRwd -339.566745\tLoss 3.869326\tAvgQ -43.304031\tMaxRwd -0.000039\n",
      "Ep 94\tStep 200\tTotRwd -242.311143\tLoss 1.684364\tAvgQ -44.598824\tMaxRwd -0.000039\n",
      "Ep 95\tStep 200\tTotRwd -376.836301\tLoss 1.829637\tAvgQ -37.678520\tMaxRwd -0.000039\n",
      "Ep 96\tStep 200\tTotRwd -249.995951\tLoss 3.187427\tAvgQ -43.290913\tMaxRwd -0.000039\n",
      "Ep 97\tStep 200\tTotRwd -277.214750\tLoss 1.433557\tAvgQ -34.229683\tMaxRwd -0.000039\n",
      "Ep 98\tStep 200\tTotRwd -0.385529\tLoss 2.758916\tAvgQ -38.111633\tMaxRwd -0.000039\n",
      "Ep 99\tStep 200\tTotRwd -1042.026717\tLoss 1.414614\tAvgQ -32.460484\tMaxRwd -0.000039\n",
      "Ep 100\tStep 200\tTotRwd -126.863119\tLoss 2.543027\tAvgQ -41.066147\tMaxRwd -0.000039\n",
      "Ep 101\tStep 200\tTotRwd -135.022892\tLoss 2.735661\tAvgQ -34.566162\tMaxRwd -0.000039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 102\tStep 200\tTotRwd -0.939706\tLoss 2.513552\tAvgQ -34.708942\tMaxRwd -0.000039\n",
      "Ep 103\tStep 200\tTotRwd -133.953930\tLoss 1.832234\tAvgQ -32.764549\tMaxRwd -0.000039\n",
      "Ep 104\tStep 200\tTotRwd -128.260421\tLoss 2.727528\tAvgQ -29.961422\tMaxRwd -0.000039\n",
      "Ep 105\tStep 200\tTotRwd -247.602069\tLoss 2.678119\tAvgQ -30.393570\tMaxRwd -0.000039\n",
      "Ep 106\tStep 200\tTotRwd -116.241418\tLoss 2.087951\tAvgQ -30.975519\tMaxRwd -0.000039\n",
      "Ep 107\tStep 200\tTotRwd -236.042055\tLoss 1.958689\tAvgQ -29.464041\tMaxRwd -0.000039\n",
      "Ep 108\tStep 200\tTotRwd -0.981178\tLoss 2.452287\tAvgQ -30.730721\tMaxRwd -0.000039\n",
      "Ep 109\tStep 200\tTotRwd -121.432196\tLoss 2.732224\tAvgQ -21.781689\tMaxRwd -0.000039\n",
      "Ep 110\tStep 200\tTotRwd -635.587977\tLoss 1.649213\tAvgQ -24.470379\tMaxRwd -0.000039\n",
      "Ep 111\tStep 200\tTotRwd -114.038825\tLoss 2.206943\tAvgQ -24.146431\tMaxRwd -0.000039\n",
      "Ep 112\tStep 200\tTotRwd -118.821673\tLoss 2.080655\tAvgQ -25.528912\tMaxRwd -0.000018\n",
      "Ep 113\tStep 200\tTotRwd -263.706617\tLoss 3.002273\tAvgQ -20.112251\tMaxRwd -0.000008\n",
      "Ep 114\tStep 200\tTotRwd -2.639908\tLoss 2.029864\tAvgQ -17.351437\tMaxRwd -0.000002\n",
      "Ep 115\tStep 200\tTotRwd -237.084330\tLoss 2.542095\tAvgQ -19.053593\tMaxRwd -0.000002\n",
      "Ep 116\tStep 200\tTotRwd -0.074035\tLoss 3.582793\tAvgQ -17.476242\tMaxRwd -0.000002\n",
      "Ep 117\tStep 200\tTotRwd -228.183420\tLoss 2.650314\tAvgQ -19.578583\tMaxRwd -0.000002\n",
      "Ep 118\tStep 200\tTotRwd -122.501320\tLoss 2.240668\tAvgQ -20.478706\tMaxRwd -0.000002\n",
      "Ep 119\tStep 200\tTotRwd -1.277291\tLoss 1.629233\tAvgQ -13.054293\tMaxRwd -0.000002\n",
      "Ep 120\tStep 200\tTotRwd -125.336254\tLoss 2.235745\tAvgQ -15.347181\tMaxRwd -0.000002\n",
      "Ep 121\tStep 200\tTotRwd -253.948201\tLoss 2.272213\tAvgQ -17.203442\tMaxRwd -0.000002\n",
      "Ep 122\tStep 200\tTotRwd -0.597491\tLoss 2.710401\tAvgQ -10.353254\tMaxRwd -0.000002\n",
      "Ep 123\tStep 200\tTotRwd -287.411761\tLoss 1.734044\tAvgQ -12.254922\tMaxRwd -0.000002\n",
      "Ep 124\tStep 200\tTotRwd -522.823621\tLoss 2.867551\tAvgQ -16.730743\tMaxRwd -0.000002\n",
      "Ep 125\tStep 200\tTotRwd -122.441414\tLoss 2.364351\tAvgQ -19.006859\tMaxRwd -0.000002\n",
      "Ep 126\tStep 200\tTotRwd -426.023027\tLoss 3.426284\tAvgQ -16.862650\tMaxRwd -0.000002\n",
      "Ep 127\tStep 200\tTotRwd -2.147689\tLoss 1.885108\tAvgQ -14.281660\tMaxRwd -0.000002\n",
      "Ep 128\tStep 200\tTotRwd -361.991703\tLoss 0.933409\tAvgQ -9.451645\tMaxRwd -0.000002\n",
      "Ep 129\tStep 200\tTotRwd -131.286220\tLoss 1.953386\tAvgQ -14.952103\tMaxRwd -0.000002\n",
      "Ep 130\tStep 200\tTotRwd -245.188604\tLoss 1.963441\tAvgQ -14.625761\tMaxRwd -0.000002\n",
      "Ep 131\tStep 200\tTotRwd -1.954979\tLoss 2.024549\tAvgQ -11.232632\tMaxRwd -0.000002\n",
      "Ep 132\tStep 200\tTotRwd -127.452198\tLoss 2.476516\tAvgQ -19.580032\tMaxRwd -0.000002\n",
      "Ep 133\tStep 200\tTotRwd -511.303983\tLoss 2.300219\tAvgQ -13.151008\tMaxRwd -0.000002\n",
      "Ep 134\tStep 200\tTotRwd -127.241246\tLoss 2.526000\tAvgQ -14.653294\tMaxRwd -0.000002\n",
      "Ep 135\tStep 200\tTotRwd -126.704738\tLoss 1.133498\tAvgQ -12.937916\tMaxRwd -0.000002\n",
      "Ep 136\tStep 200\tTotRwd -120.579653\tLoss 2.164042\tAvgQ -12.084417\tMaxRwd -0.000002\n",
      "Ep 137\tStep 200\tTotRwd -248.282874\tLoss 1.717252\tAvgQ -11.018162\tMaxRwd -0.000002\n",
      "Ep 138\tStep 200\tTotRwd -236.163047\tLoss 2.093882\tAvgQ -15.181589\tMaxRwd -0.000002\n",
      "Ep 139\tStep 200\tTotRwd -1504.123219\tLoss 1.925833\tAvgQ -18.443184\tMaxRwd -0.000002\n",
      "Ep 140\tStep 200\tTotRwd -255.236806\tLoss 2.202109\tAvgQ -17.367598\tMaxRwd -0.000002\n",
      "Ep 141\tStep 200\tTotRwd -1511.534229\tLoss 1.804628\tAvgQ -20.617868\tMaxRwd -0.000002\n",
      "Ep 142\tStep 200\tTotRwd -258.929961\tLoss 1.341658\tAvgQ -16.689049\tMaxRwd -0.000002\n",
      "Ep 143\tStep 200\tTotRwd -115.655160\tLoss 2.248561\tAvgQ -33.270142\tMaxRwd -0.000002\n",
      "Ep 144\tStep 200\tTotRwd -4.467079\tLoss 1.139591\tAvgQ -20.993919\tMaxRwd -0.000002\n",
      "Ep 145\tStep 200\tTotRwd -261.528027\tLoss 1.586588\tAvgQ -19.076607\tMaxRwd -0.000002\n",
      "Ep 146\tStep 200\tTotRwd -127.282845\tLoss 1.720294\tAvgQ -17.004902\tMaxRwd -0.000002\n",
      "Ep 147\tStep 200\tTotRwd -282.000983\tLoss 1.815491\tAvgQ -18.539120\tMaxRwd -0.000002\n",
      "Ep 148\tStep 200\tTotRwd -293.949838\tLoss 1.280975\tAvgQ -19.846512\tMaxRwd -0.000002\n",
      "Ep 149\tStep 200\tTotRwd -121.925542\tLoss 1.436200\tAvgQ -28.846367\tMaxRwd -0.000002\n",
      "Ep 150\tStep 200\tTotRwd -128.233662\tLoss 1.029931\tAvgQ -14.404372\tMaxRwd -0.000002\n",
      "Ep 151\tStep 200\tTotRwd -1.918232\tLoss 1.134549\tAvgQ -16.297205\tMaxRwd -0.000002\n",
      "Ep 152\tStep 200\tTotRwd -442.014709\tLoss 0.663907\tAvgQ -16.904263\tMaxRwd -0.000002\n",
      "Ep 153\tStep 200\tTotRwd -327.913917\tLoss 0.653600\tAvgQ -11.338772\tMaxRwd -0.000002\n",
      "Ep 154\tStep 200\tTotRwd -372.641881\tLoss 0.859811\tAvgQ -17.289478\tMaxRwd -0.000002\n",
      "Ep 155\tStep 200\tTotRwd -243.780840\tLoss 1.128391\tAvgQ -18.982714\tMaxRwd -0.000002\n",
      "Ep 156\tStep 200\tTotRwd -126.019355\tLoss 1.663258\tAvgQ -21.982929\tMaxRwd -0.000002\n",
      "Ep 157\tStep 200\tTotRwd -131.111428\tLoss 2.276269\tAvgQ -22.403122\tMaxRwd -0.000002\n",
      "Ep 158\tStep 200\tTotRwd -131.525833\tLoss 0.989190\tAvgQ -23.794460\tMaxRwd -0.000002\n",
      "Ep 159\tStep 200\tTotRwd -410.304253\tLoss 0.969981\tAvgQ -24.201803\tMaxRwd -0.000002\n",
      "Ep 160\tStep 200\tTotRwd -137.188191\tLoss 1.099868\tAvgQ -23.293640\tMaxRwd -0.000002\n",
      "Ep 161\tStep 200\tTotRwd -263.994446\tLoss 2.626258\tAvgQ -18.155195\tMaxRwd -0.000002\n",
      "Ep 162\tStep 200\tTotRwd -253.821895\tLoss 1.192004\tAvgQ -28.266621\tMaxRwd -0.000002\n",
      "Ep 163\tStep 200\tTotRwd -131.361217\tLoss 1.381003\tAvgQ -23.652563\tMaxRwd -0.000002\n",
      "Ep 164\tStep 200\tTotRwd -131.690790\tLoss 0.943739\tAvgQ -24.540087\tMaxRwd -0.000002\n",
      "Ep 165\tStep 200\tTotRwd -123.675789\tLoss 1.496007\tAvgQ -17.496712\tMaxRwd -0.000002\n",
      "Ep 166\tStep 200\tTotRwd -1.375483\tLoss 1.037277\tAvgQ -17.158375\tMaxRwd -0.000002\n",
      "Ep 167\tStep 200\tTotRwd -246.415301\tLoss 1.877380\tAvgQ -20.558372\tMaxRwd -0.000002\n",
      "Ep 168\tStep 200\tTotRwd -122.540582\tLoss 0.755348\tAvgQ -19.293056\tMaxRwd -0.000002\n",
      "Ep 169\tStep 200\tTotRwd -1.181290\tLoss 1.081054\tAvgQ -21.479939\tMaxRwd -0.000002\n",
      "Ep 170\tStep 200\tTotRwd -243.010937\tLoss 1.602656\tAvgQ -26.878035\tMaxRwd -0.000002\n",
      "Ep 171\tStep 200\tTotRwd -249.635526\tLoss 1.163222\tAvgQ -19.631409\tMaxRwd -0.000002\n",
      "Ep 172\tStep 200\tTotRwd -130.289174\tLoss 1.072009\tAvgQ -24.687334\tMaxRwd -0.000002\n",
      "Ep 173\tStep 200\tTotRwd -2.574220\tLoss 1.550499\tAvgQ -28.531591\tMaxRwd -0.000002\n",
      "Ep 174\tStep 200\tTotRwd -2.282568\tLoss 0.938511\tAvgQ -22.655193\tMaxRwd -0.000002\n",
      "Ep 175\tStep 200\tTotRwd -127.509015\tLoss 1.286026\tAvgQ -18.765369\tMaxRwd -0.000002\n",
      "Ep 176\tStep 200\tTotRwd -118.712802\tLoss 1.414039\tAvgQ -21.759819\tMaxRwd -0.000002\n",
      "Ep 177\tStep 200\tTotRwd -127.314832\tLoss 1.187499\tAvgQ -20.993038\tMaxRwd -0.000002\n",
      "Ep 178\tStep 200\tTotRwd -354.256884\tLoss 1.719804\tAvgQ -20.251892\tMaxRwd -0.000002\n",
      "Ep 179\tStep 200\tTotRwd -126.716885\tLoss 1.362095\tAvgQ -20.728218\tMaxRwd -0.000002\n",
      "Ep 180\tStep 200\tTotRwd -333.448449\tLoss 1.022498\tAvgQ -17.858435\tMaxRwd -0.000002\n",
      "Ep 181\tStep 200\tTotRwd -129.134755\tLoss 1.764923\tAvgQ -20.548290\tMaxRwd -0.000002\n",
      "Ep 182\tStep 200\tTotRwd -252.128930\tLoss 1.196950\tAvgQ -18.985365\tMaxRwd -0.000002\n",
      "Ep 183\tStep 200\tTotRwd -116.985484\tLoss 1.036680\tAvgQ -21.893520\tMaxRwd -0.000002\n",
      "Ep 184\tStep 200\tTotRwd -122.791223\tLoss 0.873977\tAvgQ -16.955078\tMaxRwd -0.000002\n",
      "Ep 185\tStep 200\tTotRwd -1.576018\tLoss 1.660685\tAvgQ -20.192484\tMaxRwd -0.000002\n",
      "Ep 186\tStep 200\tTotRwd -118.265589\tLoss 1.183954\tAvgQ -23.945023\tMaxRwd -0.000002\n",
      "Ep 187\tStep 200\tTotRwd -120.473950\tLoss 0.753676\tAvgQ -19.772974\tMaxRwd -0.000002\n",
      "Ep 188\tStep 200\tTotRwd -129.284793\tLoss 1.041950\tAvgQ -16.626808\tMaxRwd -0.000002\n",
      "Ep 189\tStep 200\tTotRwd -5.078035\tLoss 1.173111\tAvgQ -17.219324\tMaxRwd -0.000002\n",
      "Ep 190\tStep 200\tTotRwd -127.099089\tLoss 1.298313\tAvgQ -9.111115\tMaxRwd -0.000002\n",
      "Ep 191\tStep 200\tTotRwd -240.715686\tLoss 0.699842\tAvgQ -5.440066\tMaxRwd -0.000002\n",
      "Ep 192\tStep 200\tTotRwd -1.741712\tLoss 0.792017\tAvgQ -12.348021\tMaxRwd -0.000002\n",
      "Ep 193\tStep 200\tTotRwd -915.413226\tLoss 1.189970\tAvgQ -9.032162\tMaxRwd -0.000002\n",
      "Ep 194\tStep 200\tTotRwd -356.680876\tLoss 1.259004\tAvgQ -16.639801\tMaxRwd -0.000002\n",
      "Ep 195\tStep 200\tTotRwd -4.072856\tLoss 0.992085\tAvgQ -11.830947\tMaxRwd -0.000002\n",
      "Ep 196\tStep 200\tTotRwd -340.855200\tLoss 1.607427\tAvgQ -13.740494\tMaxRwd -0.000002\n",
      "Ep 197\tStep 200\tTotRwd -118.498455\tLoss 1.683749\tAvgQ -16.533270\tMaxRwd -0.000002\n",
      "Ep 198\tStep 200\tTotRwd -2.106283\tLoss 1.037731\tAvgQ -11.896608\tMaxRwd -0.000002\n",
      "Ep 199\tStep 200\tTotRwd -122.237248\tLoss 1.578321\tAvgQ -8.924835\tMaxRwd -0.000002\n",
      "Ep 200\tStep 200\tTotRwd -2.380598\tLoss 2.056740\tAvgQ -14.921143\tMaxRwd -0.000002\n",
      "Ep 201\tStep 200\tTotRwd -233.695944\tLoss 2.219296\tAvgQ -15.638582\tMaxRwd -0.000002\n",
      "Ep 202\tStep 200\tTotRwd -120.946884\tLoss 1.062152\tAvgQ -7.377975\tMaxRwd -0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 203\tStep 200\tTotRwd -230.229019\tLoss 1.247256\tAvgQ -11.246923\tMaxRwd -0.000002\n",
      "Ep 204\tStep 200\tTotRwd -339.805037\tLoss 0.939824\tAvgQ -11.264080\tMaxRwd -0.000002\n",
      "Ep 205\tStep 200\tTotRwd -1.892176\tLoss 1.369802\tAvgQ -7.071176\tMaxRwd -0.000002\n",
      "Ep 206\tStep 200\tTotRwd -122.968777\tLoss 1.232214\tAvgQ -12.738550\tMaxRwd -0.000002\n",
      "Ep 207\tStep 200\tTotRwd -1506.478559\tLoss 1.561733\tAvgQ -19.961103\tMaxRwd -0.000002\n",
      "Ep 208\tStep 200\tTotRwd -118.592165\tLoss 3.335334\tAvgQ -14.774118\tMaxRwd -0.000002\n",
      "Ep 209\tStep 200\tTotRwd -1.455667\tLoss 1.120627\tAvgQ -11.924212\tMaxRwd -0.000002\n",
      "Ep 210\tStep 200\tTotRwd -258.761004\tLoss 1.358643\tAvgQ -14.446280\tMaxRwd -0.000002\n",
      "Ep 211\tStep 200\tTotRwd -505.345098\tLoss 2.092023\tAvgQ -17.241802\tMaxRwd -0.000002\n",
      "Ep 212\tStep 200\tTotRwd -116.507288\tLoss 1.529723\tAvgQ -17.202702\tMaxRwd -0.000002\n",
      "Ep 213\tStep 200\tTotRwd -299.440746\tLoss 1.699035\tAvgQ -14.279900\tMaxRwd -0.000002\n",
      "Ep 214\tStep 200\tTotRwd -223.679415\tLoss 1.416615\tAvgQ -12.764493\tMaxRwd -0.000002\n",
      "Ep 215\tStep 200\tTotRwd -118.383134\tLoss 1.208013\tAvgQ -14.600740\tMaxRwd -0.000002\n",
      "Ep 216\tStep 200\tTotRwd -127.607656\tLoss 1.797594\tAvgQ -15.217439\tMaxRwd -0.000002\n",
      "Ep 217\tStep 200\tTotRwd -122.574113\tLoss 1.752067\tAvgQ -16.596827\tMaxRwd -0.000002\n",
      "Ep 218\tStep 200\tTotRwd -126.580774\tLoss 1.983971\tAvgQ -9.499594\tMaxRwd -0.000002\n",
      "Ep 219\tStep 200\tTotRwd -123.400387\tLoss 2.047779\tAvgQ -16.194221\tMaxRwd -0.000002\n",
      "Ep 220\tStep 200\tTotRwd -136.193261\tLoss 1.392295\tAvgQ -13.774054\tMaxRwd -0.000002\n",
      "Ep 221\tStep 200\tTotRwd -239.638027\tLoss 1.810173\tAvgQ -13.023083\tMaxRwd -0.000002\n",
      "Ep 222\tStep 200\tTotRwd -125.920127\tLoss 1.857724\tAvgQ -12.133228\tMaxRwd -0.000002\n",
      "Ep 223\tStep 200\tTotRwd -128.269725\tLoss 2.346390\tAvgQ -18.559692\tMaxRwd -0.000002\n",
      "Ep 224\tStep 200\tTotRwd -11.097004\tLoss 2.746449\tAvgQ -16.348570\tMaxRwd -0.000002\n",
      "Ep 225\tStep 200\tTotRwd -542.554605\tLoss 1.706646\tAvgQ -13.125181\tMaxRwd -0.000002\n",
      "Ep 226\tStep 200\tTotRwd -251.194164\tLoss 2.433335\tAvgQ -11.678136\tMaxRwd -0.000002\n",
      "Ep 227\tStep 200\tTotRwd -377.535865\tLoss 2.457658\tAvgQ -20.739471\tMaxRwd -0.000002\n",
      "Ep 228\tStep 200\tTotRwd -348.387686\tLoss 2.273043\tAvgQ -21.490458\tMaxRwd -0.000002\n",
      "Ep 229\tStep 200\tTotRwd -258.052839\tLoss 2.502349\tAvgQ -21.265026\tMaxRwd -0.000002\n",
      "Ep 230\tStep 200\tTotRwd -2.455806\tLoss 1.460846\tAvgQ -16.281221\tMaxRwd -0.000002\n",
      "Ep 231\tStep 200\tTotRwd -240.020296\tLoss 1.904029\tAvgQ -20.308670\tMaxRwd -0.000002\n",
      "Ep 232\tStep 200\tTotRwd -242.554456\tLoss 1.390667\tAvgQ -14.284912\tMaxRwd -0.000002\n",
      "Ep 233\tStep 200\tTotRwd -785.141994\tLoss 2.159216\tAvgQ -15.973509\tMaxRwd -0.000002\n",
      "Ep 234\tStep 200\tTotRwd -124.715539\tLoss 2.133844\tAvgQ -11.484132\tMaxRwd -0.000002\n",
      "Ep 235\tStep 200\tTotRwd -120.967016\tLoss 2.132563\tAvgQ -22.426344\tMaxRwd -0.000002\n",
      "Ep 236\tStep 200\tTotRwd -345.432545\tLoss 2.400010\tAvgQ -19.917526\tMaxRwd -0.000002\n",
      "Ep 237\tStep 200\tTotRwd -250.698927\tLoss 2.320055\tAvgQ -17.600130\tMaxRwd -0.000002\n",
      "Ep 238\tStep 200\tTotRwd -3.036213\tLoss 3.152927\tAvgQ -16.948902\tMaxRwd -0.000002\n",
      "Ep 239\tStep 200\tTotRwd -236.720268\tLoss 3.327935\tAvgQ -21.228794\tMaxRwd -0.000002\n",
      "Ep 240\tStep 200\tTotRwd -125.363320\tLoss 1.932113\tAvgQ -16.234571\tMaxRwd -0.000002\n",
      "Ep 241\tStep 200\tTotRwd -3.187484\tLoss 2.361398\tAvgQ -14.289711\tMaxRwd -0.000002\n",
      "Ep 242\tStep 200\tTotRwd -240.773648\tLoss 1.858762\tAvgQ -16.316441\tMaxRwd -0.000002\n",
      "Ep 243\tStep 200\tTotRwd -239.270811\tLoss 1.599097\tAvgQ -16.112486\tMaxRwd -0.000002\n",
      "Ep 244\tStep 200\tTotRwd -118.787578\tLoss 2.793555\tAvgQ -10.815781\tMaxRwd -0.000002\n",
      "Ep 245\tStep 200\tTotRwd -130.250798\tLoss 2.348214\tAvgQ -19.910566\tMaxRwd -0.000002\n",
      "Ep 246\tStep 200\tTotRwd -366.797700\tLoss 1.349561\tAvgQ -12.737968\tMaxRwd -0.000002\n",
      "Ep 247\tStep 200\tTotRwd -222.251291\tLoss 2.531706\tAvgQ -18.248329\tMaxRwd -0.000002\n",
      "Ep 248\tStep 200\tTotRwd -117.040464\tLoss 2.480644\tAvgQ -16.959846\tMaxRwd -0.000002\n",
      "Ep 249\tStep 200\tTotRwd -250.985229\tLoss 1.435841\tAvgQ -15.491003\tMaxRwd -0.000002\n",
      "save: ./pendulumv0.ckpt\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "episode = 0\n",
    "episode_step = 0\n",
    "noise = 0.0\n",
    "reward = np.nan\n",
    "done = True\n",
    "episode_total_reward = 0.0\n",
    "max_reward = -1e8\n",
    "critic_loss_val = np.nan\n",
    "mean_q_val = np.nan\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initialize variables\n",
    "    init.run()\n",
    "    # make the networks initially identical\n",
    "    copy_online_to_target.run()\n",
    "    # start environment\n",
    "    state = env.reset()\n",
    "    # loop\n",
    "    while episode < max_episodes:\n",
    "        # evaluate the controls\n",
    "        ctrls = actor_outputs.eval(feed_dict={states:[state]})\n",
    "        ctrls = ctrls.reshape(-1)\n",
    "        if episode < max_explore_episodes:\n",
    "             # update exploration noise \n",
    "            noise -= ((noise_theta * noise) - noise_sigma * np.random.randn())\n",
    "            # add in the exploration noise\n",
    "            p = episode / max_explore_episodes\n",
    "            ctrls = ctrls * p + (1.0 - p) * noise\n",
    "        ctrls *= control_scalar\n",
    "        # step\n",
    "        next_state, reward, done, _ = env.step(ctrls)\n",
    "        replay_memory.append((state, ctrls, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "        episode_step +=1\n",
    "        # track progress\n",
    "        episode_total_reward += reward\n",
    "        if reward > max_reward:\n",
    "            max_reward = reward\n",
    "        # if excedes the cap on steps end prematurely\n",
    "        if episode_step >= max_episode_steps:\n",
    "            done = True\n",
    "        # if episode has finished\n",
    "        if done:\n",
    "            print(\"Ep {}\\tStep {}\\tTotRwd {:2f}\\tLoss {:5f}\\tAvgQ {:2f}\\tMaxRwd {:2f}\".format(\n",
    "                episode, episode_step, episode_total_reward, critic_loss_val, mean_q_val, max_reward))\n",
    "            state = env.reset()\n",
    "            episode += 1\n",
    "            episode_step = 0\n",
    "            episode_total_reward = 0.0\n",
    "        iteration +=1\n",
    "        # if not enough replay memories\n",
    "        if iteration < min_replays:\n",
    "            # skip training\n",
    "            continue\n",
    "        # sample memories\n",
    "        mem_states, mem_controls, mem_rewards, mem_next_states, mem_continues = (sample_memories(batch_size))\n",
    "        # train the critic\n",
    "        q = target_critic_outputs.eval(feed_dict={target_states: mem_next_states})\n",
    "        y_val = mem_rewards + mem_continues * discount_rate * q\n",
    "        critic_loss_val, _ = sess.run([critic_loss, critic_training_op], feed_dict={states: mem_states,\n",
    "                                                                                    actor_outputs: mem_controls,\n",
    "                                                                                    y: y_val})\n",
    "        # train the actor\n",
    "        neg_mean_q_val, _ = sess.run([neg_mean_q, actor_training_op], feed_dict={states: mem_states})\n",
    "        mean_q_val = -1.0 * neg_mean_q_val\n",
    "        # copy to target\n",
    "        copy_online_to_target.run()\n",
    "    # save results\n",
    "    print('save:', save_path)    \n",
    "    saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./pendulumv0.ckpt\n",
      "Ep  0\n",
      "Ep  1\n",
      "Ep  2\n",
      "Ep  3\n",
      "Ep  4\n",
      "Ep  5\n",
      "Ep  6\n",
      "Ep  7\n",
      "Ep  8\n",
      "Ep  9\n",
      "Ep  10\n",
      "Ep  11\n",
      "Ep  12\n",
      "Ep  13\n",
      "Ep  14\n",
      "Ep  15\n",
      "Ep  16\n",
      "Ep  17\n",
      "Ep  18\n",
      "Ep  19\n",
      "Ep  20\n",
      "Ep  21\n",
      "Ep  22\n",
      "Ep  23\n",
      "Ep  24\n",
      "Ep  25\n",
      "Ep  26\n",
      "Ep  27\n",
      "Ep  28\n",
      "Ep  29\n",
      "Ep  30\n",
      "Ep  31\n",
      "Ep  32\n",
      "Ep  33\n",
      "Ep  34\n",
      "Ep  35\n",
      "Ep  36\n",
      "Ep  37\n",
      "Ep  38\n",
      "Ep  39\n",
      "Ep  40\n",
      "Ep  41\n",
      "Ep  42\n",
      "Ep  43\n",
      "Ep  44\n",
      "Ep  45\n",
      "Ep  46\n",
      "Ep  47\n",
      "Ep  48\n",
      "Ep  49\n",
      "Ep  50\n",
      "Ep  51\n",
      "Ep  52\n",
      "Ep  53\n",
      "Ep  54\n",
      "Ep  55\n",
      "Ep  56\n",
      "Ep  57\n",
      "Ep  58\n",
      "Ep  59\n",
      "Ep  60\n",
      "Ep  61\n",
      "Ep  62\n",
      "Ep  63\n",
      "Ep  64\n",
      "Ep  65\n",
      "Ep  66\n",
      "Ep  67\n",
      "Ep  68\n",
      "Ep  69\n",
      "Ep  70\n",
      "Ep  71\n",
      "Ep  72\n",
      "Ep  73\n",
      "Ep  74\n",
      "Ep  75\n",
      "Ep  76\n",
      "Ep  77\n",
      "Ep  78\n",
      "Ep  79\n",
      "Ep  80\n",
      "Ep  81\n",
      "Ep  82\n",
      "Ep  83\n",
      "Ep  84\n",
      "Ep  85\n",
      "Ep  86\n",
      "Ep  87\n",
      "Ep  88\n",
      "Ep  89\n",
      "Ep  90\n",
      "Ep  91\n",
      "Ep  92\n",
      "Ep  93\n",
      "Ep  94\n",
      "Ep  95\n",
      "Ep  96\n",
      "Ep  97\n",
      "Ep  98\n",
      "Ep  99\n",
      "Ep  100\n",
      "Ep  101\n",
      "Ep  102\n",
      "Ep  103\n",
      "Ep  104\n",
      "Ep  105\n",
      "Ep  106\n",
      "Ep  107\n",
      "Ep  108\n",
      "Ep  109\n",
      "Ep  110\n",
      "Ep  111\n",
      "Ep  112\n",
      "Ep  113\n",
      "Ep  114\n",
      "Ep  115\n",
      "Ep  116\n",
      "Ep  117\n",
      "Ep  118\n",
      "Ep  119\n",
      "Ep  120\n",
      "Ep  121\n",
      "Ep  122\n",
      "Ep  123\n",
      "Ep  124\n",
      "Ep  125\n",
      "Ep  126\n",
      "Ep  127\n",
      "Ep  128\n",
      "Ep  129\n",
      "Ep  130\n",
      "Ep  131\n",
      "Ep  132\n",
      "Ep  133\n",
      "Ep  134\n",
      "Ep  135\n",
      "Ep  136\n",
      "Ep  137\n",
      "Ep  138\n",
      "Ep  139\n",
      "Ep  140\n",
      "Ep  141\n",
      "Ep  142\n",
      "Ep  143\n",
      "Ep  144\n",
      "Ep  145\n",
      "Ep  146\n",
      "Ep  147\n",
      "Ep  148\n",
      "Ep  149\n",
      "Ep  150\n",
      "Ep  151\n",
      "Ep  152\n",
      "Ep  153\n",
      "Ep  154\n",
      "Ep  155\n",
      "Ep  156\n",
      "Ep  157\n",
      "Ep  158\n",
      "Ep  159\n",
      "Ep  160\n",
      "Ep  161\n",
      "Ep  162\n",
      "Ep  163\n",
      "Ep  164\n",
      "Ep  165\n",
      "Ep  166\n",
      "Ep  167\n",
      "Ep  168\n",
      "Ep  169\n",
      "Ep  170\n",
      "Ep  171\n",
      "Ep  172\n",
      "Ep  173\n",
      "Ep  174\n",
      "Ep  175\n",
      "Ep  176\n",
      "Ep  177\n",
      "Ep  178\n",
      "Ep  179\n",
      "Ep  180\n",
      "Ep  181\n",
      "Ep  182\n",
      "Ep  183\n",
      "Ep  184\n",
      "Ep  185\n",
      "Ep  186\n",
      "Ep  187\n",
      "Ep  188\n",
      "Ep  189\n",
      "Ep  190\n",
      "Ep  191\n",
      "Ep  192\n",
      "Ep  193\n",
      "Ep  194\n",
      "Ep  195\n",
      "Ep  196\n",
      "Ep  197\n",
      "Ep  198\n",
      "Ep  199\n",
      "Ep  200\n",
      "Ep  201\n",
      "Ep  202\n",
      "Ep  203\n",
      "Ep  204\n",
      "Ep  205\n",
      "Ep  206\n",
      "Ep  207\n",
      "Ep  208\n",
      "Ep  209\n",
      "Ep  210\n",
      "Ep  211\n",
      "Ep  212\n",
      "Ep  213\n",
      "Ep  214\n",
      "Ep  215\n",
      "Ep  216\n",
      "Ep  217\n",
      "Ep  218\n",
      "Ep  219\n",
      "Ep  220\n",
      "Ep  221\n",
      "Ep  222\n",
      "Ep  223\n",
      "Ep  224\n",
      "Ep  225\n",
      "Ep  226\n",
      "Ep  227\n",
      "Ep  228\n",
      "Ep  229\n",
      "Ep  230\n",
      "Ep  231\n",
      "Ep  232\n",
      "Ep  233\n",
      "Ep  234\n",
      "Ep  235\n",
      "Ep  236\n",
      "Ep  237\n",
      "Ep  238\n",
      "Ep  239\n",
      "Ep  240\n",
      "Ep  241\n",
      "Ep  242\n",
      "Ep  243\n",
      "Ep  244\n",
      "Ep  245\n",
      "Ep  246\n",
      "Ep  247\n",
      "Ep  248\n",
      "Ep  249\n",
      "Ep  250\n",
      "Ep  251\n",
      "Ep  252\n",
      "Ep  253\n",
      "Ep  254\n",
      "Ep  255\n",
      "Ep  256\n",
      "Ep  257\n",
      "Ep  258\n",
      "Ep  259\n",
      "Ep  260\n",
      "Ep  261\n",
      "Ep  262\n",
      "Ep  263\n",
      "Ep  264\n",
      "Ep  265\n",
      "Ep  266\n",
      "Ep  267\n",
      "Ep  268\n",
      "Ep  269\n",
      "Ep  270\n",
      "Ep  271\n",
      "Ep  272\n",
      "Ep  273\n",
      "Ep  274\n",
      "Ep  275\n",
      "Ep  276\n",
      "Ep  277\n",
      "Ep  278\n",
      "Ep  279\n",
      "Ep  280\n",
      "Ep  281\n",
      "Ep  282\n",
      "Ep  283\n",
      "Ep  284\n",
      "Ep  285\n",
      "Ep  286\n",
      "Ep  287\n",
      "Ep  288\n",
      "Ep  289\n",
      "Ep  290\n",
      "Ep  291\n",
      "Ep  292\n",
      "Ep  293\n",
      "Ep  294\n",
      "Ep  295\n",
      "Ep  296\n",
      "Ep  297\n",
      "Ep  298\n",
      "Ep  299\n",
      "Ep  300\n",
      "Ep  301\n",
      "Ep  302\n",
      "Ep  303\n",
      "Ep  304\n",
      "Ep  305\n",
      "Ep  306\n",
      "Ep  307\n",
      "Ep  308\n",
      "Ep  309\n",
      "Ep  310\n",
      "Ep  311\n",
      "Ep  312\n",
      "Ep  313\n",
      "Ep  314\n",
      "Ep  315\n",
      "Ep  316\n",
      "Ep  317\n",
      "Ep  318\n",
      "Ep  319\n",
      "Ep  320\n",
      "Ep  321\n",
      "Ep  322\n",
      "Ep  323\n",
      "Ep  324\n",
      "Ep  325\n",
      "Ep  326\n",
      "Ep  327\n",
      "Ep  328\n",
      "Ep  329\n",
      "Ep  330\n",
      "Ep  331\n",
      "Ep  332\n",
      "Ep  333\n",
      "Ep  334\n",
      "Ep  335\n",
      "Ep  336\n",
      "Ep  337\n",
      "Ep  338\n",
      "Ep  339\n",
      "Ep  340\n",
      "Ep  341\n",
      "Ep  342\n",
      "Ep  343\n",
      "Ep  344\n",
      "Ep  345\n",
      "Ep  346\n",
      "Ep  347\n",
      "Ep  348\n",
      "Ep  349\n",
      "Ep  350\n",
      "Ep  351\n",
      "Ep  352\n",
      "Ep  353\n",
      "Ep  354\n",
      "Ep  355\n",
      "Ep  356\n",
      "Ep  357\n",
      "Ep  358\n",
      "Ep  359\n",
      "Ep  360\n",
      "Ep  361\n",
      "Ep  362\n",
      "Ep  363\n",
      "Ep  364\n",
      "Ep  365\n",
      "Ep  366\n",
      "Ep  367\n",
      "Ep  368\n",
      "Ep  369\n",
      "Ep  370\n",
      "Ep  371\n",
      "Ep  372\n",
      "Ep  373\n",
      "Ep  374\n",
      "Ep  375\n",
      "Ep  376\n",
      "Ep  377\n",
      "Ep  378\n",
      "Ep  379\n",
      "Ep  380\n",
      "Ep  381\n",
      "Ep  382\n",
      "Ep  383\n",
      "Ep  384\n",
      "Ep  385\n",
      "Ep  386\n",
      "Ep  387\n",
      "Ep  388\n",
      "Ep  389\n",
      "Ep  390\n",
      "Ep  391\n",
      "Ep  392\n",
      "Ep  393\n",
      "Ep  394\n",
      "Ep  395\n",
      "Ep  396\n",
      "Ep  397\n",
      "Ep  398\n",
      "Ep  399\n",
      "Ep  400\n",
      "Ep  401\n",
      "Ep  402\n",
      "Ep  403\n",
      "Ep  404\n",
      "Ep  405\n",
      "Ep  406\n",
      "Ep  407\n",
      "Ep  408\n",
      "Ep  409\n",
      "Ep  410\n",
      "Ep  411\n",
      "Ep  412\n",
      "Ep  413\n",
      "Ep  414\n",
      "Ep  415\n",
      "Ep  416\n",
      "Ep  417\n",
      "Ep  418\n",
      "Ep  419\n",
      "Ep  420\n",
      "Ep  421\n",
      "Ep  422\n",
      "Ep  423\n",
      "Ep  424\n",
      "Ep  425\n",
      "Ep  426\n",
      "Ep  427\n",
      "Ep  428\n",
      "Ep  429\n",
      "Ep  430\n",
      "Ep  431\n",
      "Ep  432\n",
      "Ep  433\n",
      "Ep  434\n",
      "Ep  435\n",
      "Ep  436\n",
      "Ep  437\n",
      "Ep  438\n",
      "Ep  439\n",
      "Ep  440\n",
      "Ep  441\n",
      "Ep  442\n",
      "Ep  443\n",
      "Ep  444\n",
      "Ep  445\n",
      "Ep  446\n",
      "Ep  447\n",
      "Ep  448\n",
      "Ep  449\n",
      "Ep  450\n",
      "Ep  451\n",
      "Ep  452\n",
      "Ep  453\n",
      "Ep  454\n",
      "Ep  455\n",
      "Ep  456\n",
      "Ep  457\n",
      "Ep  458\n",
      "Ep  459\n",
      "Ep  460\n",
      "Ep  461\n",
      "Ep  462\n",
      "Ep  463\n",
      "Ep  464\n",
      "Ep  465\n",
      "Ep  466\n",
      "Ep  467\n",
      "Ep  468\n",
      "Ep  469\n",
      "Ep  470\n",
      "Ep  471\n",
      "Ep  472\n",
      "Ep  473\n",
      "Ep  474\n",
      "Ep  475\n",
      "Ep  476\n",
      "Ep  477\n",
      "Ep  478\n",
      "Ep  479\n",
      "Ep  480\n",
      "Ep  481\n",
      "Ep  482\n",
      "Ep  483\n",
      "Ep  484\n",
      "Ep  485\n",
      "Ep  486\n",
      "Ep  487\n",
      "Ep  488\n",
      "Ep  489\n",
      "Ep  490\n",
      "Ep  491\n",
      "Ep  492\n",
      "Ep  493\n",
      "Ep  494\n",
      "Ep  495\n",
      "Ep  496\n",
      "Ep  497\n",
      "Ep  498\n",
      "Ep  499\n",
      "Ep  500\n",
      "Ep  501\n",
      "Ep  502\n",
      "Ep  503\n",
      "Ep  504\n",
      "Ep  505\n",
      "Ep  506\n",
      "Ep  507\n",
      "Ep  508\n",
      "Ep  509\n",
      "Ep  510\n",
      "Ep  511\n",
      "Ep  512\n",
      "Ep  513\n",
      "Ep  514\n",
      "Ep  515\n",
      "Ep  516\n",
      "Ep  517\n",
      "Ep  518\n",
      "Ep  519\n",
      "Ep  520\n",
      "Ep  521\n",
      "Ep  522\n",
      "Ep  523\n",
      "Ep  524\n",
      "Ep  525\n",
      "Ep  526\n",
      "Ep  527\n",
      "Ep  528\n",
      "Ep  529\n",
      "Ep  530\n",
      "Ep  531\n",
      "Ep  532\n",
      "Ep  533\n",
      "Ep  534\n",
      "Ep  535\n",
      "Ep  536\n",
      "Ep  537\n",
      "Ep  538\n",
      "Ep  539\n",
      "Ep  540\n",
      "Ep  541\n",
      "Ep  542\n",
      "Ep  543\n",
      "Ep  544\n",
      "Ep  545\n",
      "Ep  546\n",
      "Ep  547\n",
      "Ep  548\n",
      "Ep  549\n",
      "Ep  550\n",
      "Ep  551\n",
      "Ep  552\n",
      "Ep  553\n",
      "Ep  554\n",
      "Ep  555\n",
      "Ep  556\n",
      "Ep  557\n",
      "Ep  558\n",
      "Ep  559\n",
      "Ep  560\n",
      "Ep  561\n",
      "Ep  562\n",
      "Ep  563\n",
      "Ep  564\n",
      "Ep  565\n",
      "Ep  566\n",
      "Ep  567\n",
      "Ep  568\n",
      "Ep  569\n",
      "Ep  570\n",
      "Ep  571\n",
      "Ep  572\n",
      "Ep  573\n",
      "Ep  574\n",
      "Ep  575\n",
      "Ep  576\n",
      "Ep  577\n",
      "Ep  578\n",
      "Ep  579\n",
      "Ep  580\n",
      "Ep  581\n",
      "Ep  582\n",
      "Ep  583\n",
      "Ep  584\n",
      "Ep  585\n",
      "Ep  586\n",
      "Ep  587\n",
      "Ep  588\n",
      "Ep  589\n",
      "Ep  590\n",
      "Ep  591\n",
      "Ep  592\n",
      "Ep  593\n",
      "Ep  594\n",
      "Ep  595\n",
      "Ep  596\n",
      "Ep  597\n",
      "Ep  598\n",
      "Ep  599\n",
      "Ep  600\n",
      "Ep  601\n",
      "Ep  602\n",
      "Ep  603\n",
      "Ep  604\n",
      "Ep  605\n",
      "Ep  606\n",
      "Ep  607\n",
      "Ep  608\n",
      "Ep  609\n",
      "Ep  610\n",
      "Ep  611\n",
      "Ep  612\n",
      "Ep  613\n",
      "Ep  614\n",
      "Ep  615\n",
      "Ep  616\n",
      "Ep  617\n",
      "Ep  618\n",
      "Ep  619\n",
      "Ep  620\n",
      "Ep  621\n",
      "Ep  622\n",
      "Ep  623\n",
      "Ep  624\n",
      "Ep  625\n",
      "Ep  626\n",
      "Ep  627\n",
      "Ep  628\n",
      "Ep  629\n",
      "Ep  630\n",
      "Ep  631\n",
      "Ep  632\n",
      "Ep  633\n",
      "Ep  634\n",
      "Ep  635\n",
      "Ep  636\n",
      "Ep  637\n",
      "Ep  638\n",
      "Ep  639\n",
      "Ep  640\n",
      "Ep  641\n",
      "Ep  642\n",
      "Ep  643\n",
      "Ep  644\n",
      "Ep  645\n",
      "Ep  646\n",
      "Ep  647\n",
      "Ep  648\n",
      "Ep  649\n",
      "Ep  650\n",
      "Ep  651\n",
      "Ep  652\n",
      "Ep  653\n",
      "Ep  654\n",
      "Ep  655\n",
      "Ep  656\n",
      "Ep  657\n",
      "Ep  658\n",
      "Ep  659\n",
      "Ep  660\n",
      "Ep  661\n",
      "Ep  662\n",
      "Ep  663\n",
      "Ep  664\n",
      "Ep  665\n",
      "Ep  666\n",
      "Ep  667\n",
      "Ep  668\n",
      "Ep  669\n",
      "Ep  670\n",
      "Ep  671\n",
      "Ep  672\n",
      "Ep  673\n",
      "Ep  674\n",
      "Ep  675\n",
      "Ep  676\n",
      "Ep  677\n",
      "Ep  678\n",
      "Ep  679\n",
      "Ep  680\n",
      "Ep  681\n",
      "Ep  682\n",
      "Ep  683\n",
      "Ep  684\n",
      "Ep  685\n",
      "Ep  686\n",
      "Ep  687\n",
      "Ep  688\n",
      "Ep  689\n",
      "Ep  690\n",
      "Ep  691\n",
      "Ep  692\n",
      "Ep  693\n",
      "Ep  694\n",
      "Ep  695\n",
      "Ep  696\n",
      "Ep  697\n",
      "Ep  698\n",
      "Ep  699\n",
      "Ep  700\n",
      "Ep  701\n",
      "Ep  702\n",
      "Ep  703\n",
      "Ep  704\n",
      "Ep  705\n",
      "Ep  706\n",
      "Ep  707\n",
      "Ep  708\n",
      "Ep  709\n",
      "Ep  710\n",
      "Ep  711\n",
      "Ep  712\n",
      "Ep  713\n",
      "Ep  714\n",
      "Ep  715\n",
      "Ep  716\n",
      "Ep  717\n",
      "Ep  718\n",
      "Ep  719\n",
      "Ep  720\n",
      "Ep  721\n",
      "Ep  722\n",
      "Ep  723\n",
      "Ep  724\n",
      "Ep  725\n",
      "Ep  726\n",
      "Ep  727\n",
      "Ep  728\n",
      "Ep  729\n",
      "Ep  730\n",
      "Ep  731\n",
      "Ep  732\n",
      "Ep  733\n",
      "Ep  734\n",
      "Ep  735\n",
      "Ep  736\n",
      "Ep  737\n",
      "Ep  738\n",
      "Ep  739\n",
      "Ep  740\n",
      "Ep  741\n",
      "Ep  742\n",
      "Ep  743\n",
      "Ep  744\n",
      "Ep  745\n",
      "Ep  746\n",
      "Ep  747\n",
      "Ep  748\n",
      "Ep  749\n",
      "Ep  750\n",
      "Ep  751\n",
      "Ep  752\n",
      "Ep  753\n",
      "Ep  754\n",
      "Ep  755\n",
      "Ep  756\n",
      "Ep  757\n",
      "Ep  758\n",
      "Ep  759\n",
      "Ep  760\n",
      "Ep  761\n",
      "Ep  762\n",
      "Ep  763\n",
      "Ep  764\n",
      "Ep  765\n",
      "Ep  766\n",
      "Ep  767\n",
      "Ep  768\n",
      "Ep  769\n",
      "Ep  770\n",
      "Ep  771\n",
      "Ep  772\n",
      "Ep  773\n",
      "Ep  774\n",
      "Ep  775\n",
      "Ep  776\n",
      "Ep  777\n",
      "Ep  778\n",
      "Ep  779\n",
      "Ep  780\n",
      "Ep  781\n",
      "Ep  782\n",
      "Ep  783\n",
      "Ep  784\n",
      "Ep  785\n",
      "Ep  786\n",
      "Ep  787\n",
      "Ep  788\n",
      "Ep  789\n",
      "Ep  790\n",
      "Ep  791\n",
      "Ep  792\n",
      "Ep  793\n",
      "Ep  794\n",
      "Ep  795\n",
      "Ep  796\n",
      "Ep  797\n",
      "Ep  798\n",
      "Ep  799\n",
      "Ep  800\n",
      "Ep  801\n",
      "Ep  802\n",
      "Ep  803\n",
      "Ep  804\n",
      "Ep  805\n",
      "Ep  806\n",
      "Ep  807\n",
      "Ep  808\n",
      "Ep  809\n",
      "Ep  810\n",
      "Ep  811\n",
      "Ep  812\n",
      "Ep  813\n",
      "Ep  814\n",
      "Ep  815\n",
      "Ep  816\n",
      "Ep  817\n",
      "Ep  818\n",
      "Ep  819\n",
      "Ep  820\n",
      "Ep  821\n",
      "Ep  822\n",
      "Ep  823\n",
      "Ep  824\n",
      "Ep  825\n",
      "Ep  826\n",
      "Ep  827\n",
      "Ep  828\n",
      "Ep  829\n",
      "Ep  830\n",
      "Ep  831\n",
      "Ep  832\n",
      "Ep  833\n",
      "Ep  834\n",
      "Ep  835\n",
      "Ep  836\n",
      "Ep  837\n",
      "Ep  838\n",
      "Ep  839\n",
      "Ep  840\n",
      "Ep  841\n",
      "Ep  842\n",
      "Ep  843\n",
      "Ep  844\n",
      "Ep  845\n",
      "Ep  846\n",
      "Ep  847\n",
      "Ep  848\n",
      "Ep  849\n",
      "Ep  850\n",
      "Ep  851\n",
      "Ep  852\n",
      "Ep  853\n",
      "Ep  854\n",
      "Ep  855\n",
      "Ep  856\n",
      "Ep  857\n",
      "Ep  858\n",
      "Ep  859\n",
      "Ep  860\n",
      "Ep  861\n",
      "Ep  862\n",
      "Ep  863\n",
      "Ep  864\n",
      "Ep  865\n",
      "Ep  866\n",
      "Ep  867\n",
      "Ep  868\n",
      "Ep  869\n",
      "Ep  870\n",
      "Ep  871\n",
      "Ep  872\n",
      "Ep  873\n",
      "Ep  874\n",
      "Ep  875\n",
      "Ep  876\n",
      "Ep  877\n",
      "Ep  878\n",
      "Ep  879\n",
      "Ep  880\n",
      "Ep  881\n",
      "Ep  882\n",
      "Ep  883\n",
      "Ep  884\n",
      "Ep  885\n",
      "Ep  886\n",
      "Ep  887\n",
      "Ep  888\n",
      "Ep  889\n",
      "Ep  890\n",
      "Ep  891\n",
      "Ep  892\n",
      "Ep  893\n",
      "Ep  894\n",
      "Ep  895\n",
      "Ep  896\n",
      "Ep  897\n",
      "Ep  898\n",
      "Ep  899\n",
      "Ep  900\n",
      "Ep  901\n",
      "Ep  902\n",
      "Ep  903\n",
      "Ep  904\n",
      "Ep  905\n",
      "Ep  906\n",
      "Ep  907\n",
      "Ep  908\n",
      "Ep  909\n",
      "Ep  910\n",
      "Ep  911\n",
      "Ep  912\n",
      "Ep  913\n",
      "Ep  914\n",
      "Ep  915\n",
      "Ep  916\n",
      "Ep  917\n",
      "Ep  918\n",
      "Ep  919\n",
      "Ep  920\n",
      "Ep  921\n",
      "Ep  922\n",
      "Ep  923\n",
      "Ep  924\n",
      "Ep  925\n",
      "Ep  926\n",
      "Ep  927\n",
      "Ep  928\n",
      "Ep  929\n",
      "Ep  930\n",
      "Ep  931\n",
      "Ep  932\n",
      "Ep  933\n",
      "Ep  934\n",
      "Ep  935\n",
      "Ep  936\n",
      "Ep  937\n",
      "Ep  938\n",
      "Ep  939\n",
      "Ep  940\n",
      "Ep  941\n",
      "Ep  942\n",
      "Ep  943\n",
      "Ep  944\n",
      "Ep  945\n",
      "Ep  946\n",
      "Ep  947\n",
      "Ep  948\n",
      "Ep  949\n",
      "Ep  950\n",
      "Ep  951\n",
      "Ep  952\n",
      "Ep  953\n",
      "Ep  954\n",
      "Ep  955\n",
      "Ep  956\n",
      "Ep  957\n",
      "Ep  958\n",
      "Ep  959\n",
      "Ep  960\n",
      "Ep  961\n",
      "Ep  962\n",
      "Ep  963\n",
      "Ep  964\n",
      "Ep  965\n",
      "Ep  966\n",
      "Ep  967\n",
      "Ep  968\n",
      "Ep  969\n",
      "Ep  970\n",
      "Ep  971\n",
      "Ep  972\n",
      "Ep  973\n",
      "Ep  974\n",
      "Ep  975\n",
      "Ep  976\n",
      "Ep  977\n",
      "Ep  978\n",
      "Ep  979\n",
      "Ep  980\n",
      "Ep  981\n",
      "Ep  982\n",
      "Ep  983\n",
      "Ep  984\n",
      "Ep  985\n",
      "Ep  986\n",
      "Ep  987\n",
      "Ep  988\n",
      "Ep  989\n",
      "Ep  990\n",
      "Ep  991\n",
      "Ep  992\n",
      "Ep  993\n",
      "Ep  994\n",
      "Ep  995\n",
      "Ep  996\n",
      "Ep  997\n",
      "Ep  998\n",
      "Ep  999\n",
      "Ep  1000\n",
      "Ep  1001\n",
      "Ep  1002\n",
      "Ep  1003\n",
      "Ep  1004\n",
      "Ep  1005\n",
      "Ep  1006\n",
      "Ep  1007\n",
      "Ep  1008\n",
      "Ep  1009\n",
      "Ep  1010\n",
      "Ep  1011\n",
      "Ep  1012\n",
      "Ep  1013\n",
      "Ep  1014\n",
      "Ep  1015\n",
      "Ep  1016\n",
      "Ep  1017\n",
      "Ep  1018\n",
      "Ep  1019\n",
      "Ep  1020\n",
      "Ep  1021\n",
      "Ep  1022\n",
      "Ep  1023\n"
     ]
    }
   ],
   "source": [
    "steps = []\n",
    "cumulative_rewards = []\n",
    "# run simulations with the saved model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_path)\n",
    "    # for each simulation\n",
    "    for episode in range(1024):\n",
    "        done = False\n",
    "        cumulative_reward = 0.0\n",
    "        step = 0\n",
    "        state = env.reset()\n",
    "        # loop\n",
    "        print('Ep ', episode)\n",
    "        while not done:\n",
    "            ctrls = actor_outputs.eval(feed_dict={states:[state]})\n",
    "            ctrls = ctrls.reshape(-1)\n",
    "            ctrls *= control_scalar\n",
    "            state, reward, done, _ = env.step(ctrls)\n",
    "            cumulative_reward += reward\n",
    "            step += 1\n",
    "        steps.append(step)\n",
    "        cumulative_rewards.append(cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   0.,   0.,   2.,   1.,   4.,  15., 102., 202., 696.]),\n",
       " array([-1532.8197729 , -1379.96844815, -1227.1171234 , -1074.26579866,\n",
       "         -921.41447391,  -768.56314916,  -615.71182442,  -462.86049967,\n",
       "         -310.00917492,  -157.15785018,    -4.30652543]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram\n",
    "plt.hist(np.array(cumulative_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.,    0.,    0.,    0.,    0., 1024.,    0.,    0.,    0.,\n",
       "           0.]),\n",
       " array([199.5, 199.6, 199.7, 199.8, 199.9, 200. , 200.1, 200.2, 200.3,\n",
       "        200.4, 200.5]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram\n",
    "plt.hist(np.array(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
